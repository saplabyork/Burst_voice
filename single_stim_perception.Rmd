---
title: "Single stim perception analysis"
output: 
  html_document:
    includes:
      in_header: "favicon.html"
    theme: paper
    toc: true
    toc_float: true
    collapsed: false
    number_sections: false
    toc_depth: 3
---

<style type="text/css">
  body{
  font-size: 12pt;
}
</style>

<style type="text/css">
.title {
  display: none;
}

#getting-started img {
  margin-right: 10px;
}

</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(repos = c(CRAN = "https://cran.rstudio.com/"))
install.packages("ggeffects")
```


```{r}
per_data <- read.csv("/Users/chandannarayan/GitHub/Burst_voice/Data/Master_Single_stim_long.csv", header=TRUE)

number_of_subs <- length(unique(per_data$sub))
number_of_subs

library(lme4)
library(ggplot2)
library(ggeffects)

per_data$ons_con <- as.factor(per_data$ons_con)

#per_data_cor <- subset(per_data, Correct=="1")

library(tidyverse)

# Create a crosstab table with ons_con on rows, stim_length on columns
# with marginal totals for each row and column

crosstab_table <- per_data %>%
  group_by(ons_con, stim_length) %>%
  summarise(
    percent_correct = mean(Correct) * 100,
    n = n(),
    .groups = "drop"
  ) %>%
  # Add formatted column combining percentage and n
  mutate(formatted = paste0(round(percent_correct, 1), "%\n(n=", n, ")")) %>%
  select(ons_con, stim_length, formatted) %>%
  # Pivot wider to get stim_length as columns
  pivot_wider(names_from = stim_length, values_from = formatted)

# Calculate row totals (marginal totals for each ons_con)
row_totals <- per_data %>%
  group_by(ons_con) %>%
  summarise(
    percent_correct = mean(Correct) * 100,
    n = n(),
    .groups = "drop"
  ) %>%
  mutate(Total = paste0(round(percent_correct, 1), "%\n(n=", n, ")")) %>%
  select(ons_con, Total)

# Calculate column totals (marginal totals for each stim_length)
col_totals <- per_data %>%
  group_by(stim_length) %>%
  summarise(
    percent_correct = mean(Correct) * 100,
    n = n(),
    .groups = "drop"
  ) %>%
  mutate(formatted = paste0(round(percent_correct, 1), "%\n(n=", n, ")")) %>%
  select(stim_length, formatted) %>%
  pivot_wider(names_from = stim_length, values_from = formatted) %>%
  mutate(ons_con = "Total")

# Calculate overall total
overall_total <- per_data %>%
  summarise(
    percent_correct = mean(Correct) * 100,
    n = n()
  ) %>%
  mutate(Total = paste0(round(percent_correct, 1), "%\n(n=", n, ")")) %>%
  pull(Total)

# Add row totals to the main table
final_table <- crosstab_table %>%
  left_join(row_totals, by = "ons_con")

# Add column totals row
col_totals_row <- col_totals %>%
  mutate(Total = overall_total)

# Combine everything
complete_table <- bind_rows(
  final_table,
  col_totals_row
)

# Display the table
print(complete_table)

# Alternative: Create a cleaner version without the \n formatting
clean_table <- per_data %>%
  group_by(ons_con, stim_length) %>%
  summarise(
    percent_correct = round(mean(Correct) * 100, 1),
    .groups = "drop"
  ) %>%
  pivot_wider(names_from = stim_length, values_from = percent_correct) %>%
  # Add row totals
  left_join(
    per_data %>%
      group_by(ons_con) %>%
      summarise(Total = round(mean(Correct) * 100, 1), .groups = "drop"),
    by = "ons_con"
  )

# Add column totals
col_totals_clean <- per_data %>%
  group_by(stim_length) %>%
  summarise(percent_correct = round(mean(Correct) * 100, 1), .groups = "drop") %>%
  pivot_wider(names_from = stim_length, values_from = percent_correct) %>%
  mutate(
    ons_con = "Total",
    Total = round(mean(per_data$Correct) * 100, 1)
  )

clean_complete_table <- bind_rows(clean_table, col_totals_clean)

print("\nClean version (percentages only):")
print(clean_complete_table)


# Fit the mixed-effects logistic regression model with interaction term
per_model <- glmer(response_vc ~ stim_length * ons_con + (1 | vowel) + (1 | sub), 
                   data = per_data, 
                   family = binomial)

summary(per_model)

# Fully crossed model with random slopes/intercepts
per_model_full <- glmer(response_vc ~ stim_length * ons_con + (1 + stim_length + ons_con | sub), 
                   data = per_data, 
                   family = binomial, control=glmerControl(optimizer="bobyqa",
                            optCtrl=list(maxfun=2e5)))

summary(per_model_full)

# Singularity problem with random effects structure, so random slopes/intercepts for just one variable

per_model_full <- glmer(response_vc ~ stim_length * ons_con + (1 + stim_length | sub), 
                   data = per_data, 
                   family = binomial, control=glmerControl(optimizer="bobyqa",
                            optCtrl=list(maxfun=2e5)))

summary(per_model_full)


# Generate predicted probabilities
per_data$predicted <- predict(per_model_full, type = "response")  # Predicted probabilities

# Summarize predicted probabilities by `stim_length` and `ons_con`
summary_df <- aggregate(predicted ~ stim_length + ons_con, data = per_data, 
                        FUN = function(x) c(mean = mean(x), se = sd(x)/sqrt(length(x))))

# Unpack the summary into a new data frame
summary_df <- do.call(data.frame, summary_df)
colnames(summary_df) <- c("stim_length", "ons_con", "mean_prob", "se_prob")

# Compute confidence intervals
summary_df$lower_ci <- summary_df$mean_prob - 1.96 * summary_df$se_prob
summary_df$upper_ci <- summary_df$mean_prob + 1.96 * summary_df$se_prob

# Plot the results with facets for ons_con
ggplot(summary_df, aes(x = stim_length, y = mean_prob)) +
  geom_point(size = 2) +
  geom_errorbar(aes(ymin = lower_ci, ymax = upper_ci), width = 0.2) +
  facet_wrap(~ons_con) +  # Create separate facets for each level of ons_con
  scale_y_continuous(limits = c(0.1, 0.5)) +  # Set y-axis limits from 0.1 to 0.5
  labs(#title = "Predicted Probabilities by Stimulus Length and Onset Consonant",
       x = "Onset VOT Length",
       y = "Predicted Probability of Voiced Coda") +
  theme_bw(base_size = 12)

# Plot the results with color-coded ons_con
ggplot(summary_df, aes(x = stim_length, y = mean_prob, color = ons_con)) +
  geom_point(size = 4) +
  geom_errorbar(aes(ymin = lower_ci, ymax = upper_ci), width = 0.2) +
  scale_y_continuous(limits = c(0.1, 0.6)) +  # Set y-axis limits from 0 to 1
  labs(title = "Predicted Probabilities by Stimulus Length and Onset Consonant",
       x = "VOT Length",
       y = "Predicted Probability of Voiced Coda",
       color = "Onset Consonant") +
  theme_minimal()


###

# Install ggeffects if not already installed
install.packages("ggeffects")

# Load the package
library(ggeffects)

# Generate predicted probabilities for `stim_length`
probabilities <- ggpredict(per_model, terms = "stim_length")

# Print the probabilities
print(probabilities)

# Plot the probabilities
plot(probabilities)


```

```{r}
# Install if needed
# install.packages("brms")

# Trying out Bayes

library(brms)

per_model_bayes <- brm(response_vc ~ stim_length * ons_con + (1 + stim_length | sub), 
                       data = per_data, 
                       family = bernoulli(),
                       chains = 4,
                       iter = 2000,
                       cores = 4)

# Check the model
summary(per_model_bayes)
plot(per_model_bayes)


# Check the coding of stim_length
table(per_data$stim_length)
levels(factor(per_data$stim_length))

# Check what the reference level is
contrasts(factor(per_data$stim_length))


## Get the actual probabilities from the model

# Extract coefficients
fixef(per_model_bayes)

# For long stimuli, ons_con at reference level:
intercept <- -0.34
prob_long_ref <- plogis(intercept)  # plogis converts log-odds to probability
prob_long_ref  # Should be around 0.42

# For short stimuli, ons_con at reference level:
prob_short_ref <- plogis(intercept + (-1.34))
prob_short_ref  # Should be around 0.17

# For long stimuli, ons_con = "p":
prob_long_p <- plogis(intercept + (-0.08))
prob_long_p

# For short stimuli, ons_con = "p":
prob_short_p <- plogis(intercept + (-1.34) + (-0.08) + 1.07)
prob_short_p


# Load required libraries
library(brms)
library(ggplot2)
library(dplyr)
library(tidyr)

# Create a grid of predictor values to get estimates for each condition
# Get unique values of your predictors
stim_lengths <- unique(per_data$stim_length)
ons_cons <- unique(per_data$ons_con)

# Create prediction grid
pred_grid <- expand.grid(
  stim_length = stim_lengths,
  ons_con = ons_cons
)

# Get posterior predictions for each condition
# Get the full posterior draws first
posterior_draws <- posterior_epred(per_model_bayes, 
                                  newdata = pred_grid,
                                  allow_new_levels = TRUE,
                                  re_formula = NA)  # This marginalizes over random effects

# Calculate summary statistics manually
posterior_summary <- apply(posterior_draws, 2, function(x) {
  c(estimate = mean(x),
    lower = quantile(x, 0.025),
    upper = quantile(x, 0.975))
})

# Combine predictions with the grid
plot_data <- pred_grid %>%
  mutate(
    estimate = posterior_summary[1, ],
    lower = posterior_summary[2, ],
    upper = posterior_summary[3, ]
  )

# Create the plot
ggplot(plot_data, aes(x = factor(ons_con, levels = c("p", "k")), y = estimate, color = factor(stim_length))) +
  geom_point(size = 3, position = position_dodge(width = 0.1)) +
  geom_errorbar(aes(ymin = lower, ymax = upper), 
                width = 0.05, 
                size = 1,
                position = position_dodge(width = 0.1)) +
  labs(
    title = "Bayesian Model Estimates by Stimulus Length and Onset Condition",
    x = "Onset Condition",
    y = "Predicted probability of a perceived VOICED coda",
    color = "Stimulus Length"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    plot.title = element_text(hjust = 0.5, size = 14),
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 12),
    legend.text = element_text(size = 11),
    panel.grid.major = element_line(color = "grey90", size = 0.5),
    panel.grid.minor = element_line(color = "grey95", size = 0.3)
  ) +
  scale_y_continuous(limits = c(0, 1), labels = scales::percent_format())

# Alternative version with different styling
ggplot(plot_data, aes(x = factor(ons_con, levels = c("p", "k")), y = estimate, color = factor(stim_length))) +
  geom_point(size = 4, position = position_dodge(width = 0.3)) +
  geom_errorbar(aes(ymin = lower, ymax = upper), 
                width = 0.2, 
                size = 1.2,
                position = position_dodge(width = 0.3)) +
  labs(
    title = "Model Estimates with 95% Credible Intervals",
    x = "Onset",
    y = "Predicted probability of a perceived VOICED coda",
    color = "Stimulus Length"
  ) +
  theme_classic() +
  theme(
    legend.position = "right",
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 14),
    legend.title = element_text(size = 12, face = "bold"),
    legend.text = element_text(size = 11),
    panel.grid.major = element_line(color = "grey90", size = 0.5),
    panel.grid.minor = element_line(color = "grey95", size = 0.3)
  ) +
  scale_y_continuous(limits = c(0, 1), labels = scales::percent_format())

# If you want to see the actual data values used:
print("Prediction grid and estimates:")
print(plot_data)

```

# How to interpret the model and plot?

The negative coefficient for stim_lengthshort (-1.33) means that short stimulus length decreases the probability that response_vc = 1
In other words: long stimuli have higher probability of voiced coda perception than short stimuli

This matches your plot perfectly - you can see that the points for long stimuli (one color) are higher on the y-axis than the points for short stimuli (other color) for both onset conditions.
In practical terms:

Long stimuli → higher probability of perceiving a voiced coda
Short stimuli → lower probability of perceiving a voiced coda
The interaction shows this length effect is stronger for /k/ onsets than /p/ onsets

# Key diagnostics you should report:

```{r}
# # Load required libraries
# library(brms)        # Load brms first
# library(ggplot2)
# library(bayesplot)   # Load bayesplot after brms
# rhat <- brms::rhat
# 
# # 1. CONVERGENCE DIAGNOSTICS (Most Important)
# # Check Rhat values (should be < 1.01, ideally < 1.001)
# rhat_values <- rhat(per_model_bayes)
# print("Rhat values:")
# print(rhat_values)
# 
# # Check effective sample sizes
# #ess_bulk <- neff_ratio(per_model_bayes)
# # For tail ESS, check the summary or use posterior package
# # Check convergence diagnostics
# summary(per_model_bayes)  # This shows Bulk_ESS and Tail_ESS in the output
# model_summary <- summary(per_model_bayes)
# print("Effective Sample Size ratios (should be > 0.1):")
# print(paste("Bulk ESS ratio range:", round(min(ess_bulk), 3), "to", round(max(ess_bulk), 3)))
# print(paste("Tail ESS ratio range:", round(min(ess_tail), 3), "to", round(max(ess_tail), 3)))
# 
# # 2. TRACE PLOTS (Visual convergence check)
# plot(per_model_bayes, pars = c("b_Intercept", "b_stim_lengthshort", 
#                                "b_ons_conp", "b_stim_lengthshort:ons_conp"))
# 
# # 3. POSTERIOR PREDICTIVE CHECKS
# # Check if model reproduces key patterns in the data
# pp_check(per_model_bayes, ndraws = 100)
# 
# # More specific posterior predictive checks
# pp_check(per_model_bayes, type = "bars", ndraws = 500)  # For binary data
# pp_check(per_model_bayes, type = "stat", stat = "mean", ndraws = 1000)
# pp_check(per_model_bayes, type = "stat_grouped", 
#          stat = "mean", group = "stim_length", ndraws = 500)
# 
# # 4. MODEL COMPARISON (if you have alternative models)
# # Example: Compare to simpler models
# # per_model_simple <- brm(response_vc ~ stim_length + ons_con + (1|sub), 
# #                        data = per_data, family = bernoulli())
# # loo_compare(loo(per_model_bayes), loo(per_model_simple))
# 
# # 5. RESIDUAL DIAGNOSTICS
# # For mixed effects models, check residuals at different levels
# residuals_plot <- plot(per_model_bayes, type = "residuals")
# print(residuals_plot)
# 
# # 6. RANDOM EFFECTS DIAGNOSTICS
# # Check if random effects are reasonable
# plot(per_model_bayes, pars = "^r_sub")
# 
# # Check random effects correlations
# plot(per_model_bayes, pars = "cor")
# 
# # 7. MCMC DIAGNOSTICS
# # Check for divergent transitions
# nuts_params <- nuts_params(per_model_bayes)
# print("Number of divergent transitions:")
# sum(subset(nuts_params, Parameter == "divergent__")$Value)
# 
# # Energy diagnostics
# mcmc_nuts_energy(nuts_params)
# 
# # 8. MODEL SUMMARY FOR REPORTING
# print("=== MODEL SUMMARY FOR REPORTING ===")
# summary(per_model_bayes)
# 
# # 9. LEAVE-ONE-OUT CROSS-VALIDATION
# loo_result <- loo(per_model_bayes)
# print(loo_result)
# 
# # Check for problematic observations
# plot(loo_result)
# 
# # 10. PRIOR-POSTERIOR COMPARISON (if you set informative priors)
# # Check if priors were reasonable
# prior_summary(per_model_bayes)
# 
# # 11. R-SQUARED (Bayesian version)
# bayes_R2(per_model_bayes)
# 
# # 12. WHAT TO REPORT IN YOUR PAPER
# cat("
# === DIAGNOSTICS TO REPORT ===
# 
# 1. CONVERGENCE:
#    - All Rhat values < 1.01 (report range)
#    - Effective sample sizes > 400 for all parameters
#    - No divergent transitions
#    
# 2. MODEL FIT:
#    - Posterior predictive checks show model reproduces data patterns
#    - LOO-CV information criterion for model comparison
#    - Bayesian R-squared for explained variance
#    
# 3. SAMPLE DESCRIPTION:
#    - 4 chains, 2000 iterations each (1000 warmup)
#    - Total of 4000 post-warmup draws
#    - MCMC sampling using NUTS algorithm
# 
# Example text for methods:
# 'Model convergence was assessed using Rhat statistics (all < 1.01) 
# and effective sample sizes (all > 400). Posterior predictive checks 
# confirmed the model adequately reproduced the observed data patterns. 
# No divergent transitions were observed during sampling.'
# ")
# 
# # 13. SPECIFIC CHECKS FOR YOUR MODEL
# # Check interaction effects visually
# marginal_effects(per_model_bayes, effects = "stim_length:ons_con")
# 
# # Check if random effects structure is justified
# # Compare models with/without random slopes
# # This helps justify the complexity of your random effects
```

1. Convergence (Essential)

Rhat < 1.01 for all parameters (yours look good: all ≈ 1.00)
Effective sample sizes > 400 (yours are excellent: 862-4104)
No divergent transitions

2. Model Fit

Posterior predictive checks (pp_check()) - shows if model reproduces data patterns
LOO cross-validation for model comparison
Bayesian R² for explained variance

3. What to report in your paper:
"Model convergence was assessed using R̂ statistics (all < 1.01, range: 1.00-1.01) and effective sample sizes (all > 400, range: 862-4104). Posterior predictive checks confirmed adequate model fit to the observed data. MCMC sampling used 4 chains with 2000 iterations each (1000 warmup), yielding 4000 post-warmup draws with no divergent transitions."
Your model already shows excellent diagnostics:

Perfect Rhat values (all 1.00-1.01)
High effective sample sizes
Good mixing indicated by the diagnostics

The most important ones to run and report are the convergence checks (which you already have) and posterior predictive checks to show your model actually fits the data well.